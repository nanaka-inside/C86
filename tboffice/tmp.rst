
*****
chap1
*****


Immutable Infrastructureの最適解を探る
=====================================

本特集では、2014年のインフラ界のバズワードである、「Immutable Infrastructure」について取り上げます。
IIはどういうものなのか、また、実際に行うためのソフトウエアについても幅広く触れていきたいと思います。


失敗のないデプロイを目指して
-------------------------

IIとはどういうものなのか、というところから始まると思った？残念！まず結論です！ [#iizannen]_
IIは、 **失敗のないデプロイを行うための一つの考え方** です。IIについていろいろ調べていると、一冊の本に行き当たります。
「継続的デリバリー 信頼できるソフトウェアリリースのためのビルド・テスト・デプロイメントの自動化」 [#iikz]_ です。
この本には、ソフトウエアをユーザにできるだけ早く届ける方法が書かれています。コードのテストの自動化から、素早い本番環境へのデプロイ方法になど、幅広く書かれています。

さて、この本は Martin Fowler 氏がまえがきを書いています。そのMartin Fowler氏のブログに2013年6月、
CHad Fowler氏 [#iichad]_ が「サーバを捨てて、コードを焼き付けろ！」(Trash Your Servers and Burn Your Code) [#iitys]_ というエントリを投稿しています [#iihottan]_ [#iifow]_ 。


.. [#iizannen] 残念さやかちゃん
.. [#iikz] http://www.amazon.co.jp/dp/4048707876
.. [#iifow] どちらもFowlerですが、たまたま名前が同じだった件

ここに、興味深い示唆があるのでJunichi Niino氏の邦訳 [#iihottan]_ から引用してみましょう。

  開発者として、あるいはしばしばシステム管理をする者として、これまで経験したもっとも恐ろしいものの1つは、長年にわたり稼働し続けてなんどもシステムやアプリケーションのアップグレードを繰り返してきたサーバだ。
  なぜか。その理由は、古いシステムはつぎはぎのような処置がされているに違いないからだ。障害が起きたときに一時しのぎのハックで対処され、コンフィグファイルをちょこっと直してやり過ごしてしまう。「あとでChefの方に反映しておくよ」とそのときは言うけれど、炎上したシステムの対処に疲れて一眠りしたあとでは、そんなことは忘れてしまうだろう。
  予想もしないところでcronのジョブが走り始めて、よく分からないけれどなにか大事な処理をしていて、そのことを知っているのは関係者のうちの1人だけとか。通常のソースコード管理システムを使わずにコードが直接書き換えられているとか。システムがどんどん扱いにくくなっていき、手作業でしかデプロイできなくなるとか。初期化スクリプトがもはや、思いも付かないような例外的な操作をしなければ動かなくなっているとか。
  もちろんOSは（きちんと管理されているならば）なんども適切にパッチが当て続けられているだろうが、そこには（訳注：やがて秩序が失われていくという）エントロピーの法則が忍び込んでくるものだし、適切に管理されていないとすればいちどもパッチは当てられず、もしこれからパッチを当てようものならなにが起きるか分からない。
  私たちはこの問題を解決するために何年ものあいだ、チームポリシーの策定から自動化までさまざまな方法を試してきた。そしていま試している新しい方法が「Immutable Deployments」（イミュータブル・デプロイメント）だ。

.. [#iikief] http://kief.com/, https://twitter.com/kief
.. [#iiims] http://martinfowler.com/bliki/ImmutableServer.html
.. [#iichad] https://twitter.com/chadfowler
.. [#iitys] 邦題は@naoya氏の「Immutable Infrastructure Conference #1」の発言から引用。「Trash Your Servers and Burn Your Code: Immutable Infrastructure and Disposable Components」http://chadfowler.com/blog/2013/06/23/immutable-deployments/
.. [#iihottan] このへんの流れは、 Junichi Niino氏の「『Immutable Infrastructure（イミュータブルインフラストラクチャ）と捨ててしまえるコンポーネント』 チャド・ファウラー氏」http://www.publickey1.jp/blog/14/immutable_infrastructure.html　を参考にしました。っていうかほぼそのまま


このエントリの10日前に Kief Morris氏 [#iikief]_ が「ImmutableServer」 [#iiims]_ というエントリを投稿しています。このエントリでは、サーバを作って壊すという方法が提案されています。


作って壊す、そして自動化
----------------------

Martin Fowler氏のブログに、PhenixServer [#iifs]_ という記事があります。不死鳥のように蘇るサーバという意味です。
動作中のサーバの監査のお仕事をするために、本番と同じサーバをつくろうともったら、構成のズレやアドホックな変更でサーバの設定が漂流していたのだそうです [#iisfs]_ 。
だったらいっそのこと定期的にサーバを焼き払ったほうがよく、puppetやchefを使ってサーバを作り直そうと書かれています。

.. [#iifs] http://martinfowler.com/bliki/PhoenixServer.html
.. [#iisfs] そんなサーバのことを SnowflakeServer(雪の欠片サーバ) という http://martinfowler.com/bliki/SnowflakeServer.html

別の例を挙げると、開発環境をいじくりまくって、やっぱりもとの綺麗さっぱりした状態にもどしたい、なんて経験は一回や二回、いやもっとあったかな？
そんなときに、作りなおすことが簡単にできたらとてもうれしいですよね。先ほどでてきた「継続的デリバリー」の中でも重要な事として **自動化** が何度も登場します。

自動化を推し進めると、コードのテストから、バグの修正や機能の拡張を本番サーバにデプロイするまでがほぼ自動となり、デプロイの回数を安全に増やすことができます。

2012年に行われたカンファレンス、AWS re:Inventにて「Amazonは1時間に最大1000回もデプロイする」 [#iideploy]_ という公演がありました。
そのなかで、「Amazon.comでは11秒ごとに新しいコードがデプロイされている。そして最も多いときで1時間に1079回デプロイが行われた。
これには機能追加だけでなくバグフィクスなども含まれるが。平均で1万、最大で3万ものホストがデプロイを受け取る」とあります。
これは、バグはすぐに潰され、機能の追加の恩恵も受けられることを示します。このサイクルを行うために、継続的デリバリーでも強調されている **自動化** が必須となります。

例えば、この本の原稿の生成も自動化されています [#iikonohon]_ 。
githubにReST形式の原稿をpushすると、それを検知したjenkinsがsphinx [#iisphinx]_ のコマンドを実行し、入稿用のPDFが生成されます。

自動化の最先端として、githubにpull requestを行うとテストが実行され、そのあと本番環境へデプロイされる仕組みが@naoya氏のブログで紹介されています [#iighedep]_ 。
pull requiestをIRCなどのツールで自動化して作成し、Pull Request内容を確認、mergeするとそのままテストが走り、そして本番環境へコードが入ります。
自動化できるところは自動化しましょう。人的ミスがなくなります。

.. [#iideploy] http://www.publickey1.jp/blog/12/amazon11000_aws_reinventday2_am.html
.. [#iisphinx] ドキュメントビルダーのsphinxです。http://sphinx-users.jp/
.. [#iighedep] GitHub 時代のデプロイ戦略 http://d.hatena.ne.jp/naoya/20140502/1399027655
.. [#iikonohon] ななかInsidePRESS vol.1では原稿はGitHubにあり、PDFは手動でビルドしていました 
.. [#iivps] Virtual Private Server。仮想専用サーバのことです。この原稿PDFはさくらのVPSでビルドされています


Immutable Infrastructure とは
-----------------------------

ようやく本題に入ることが出来ます。IIは、直訳すると、「不変なインフラ」となります。一度デプロイしたサーバには変更を加えないということを意味しています。
もし、アプリケーションに変更を加えたい場合は、再度サーバを構築すればいいのです。そして以前デプロイしたサーバは、綺麗さっぱり捨てます [#iidi]_。

.. [#iidi] サーバ以外のアプリケーションなどのコンポーネントも綺麗さっぱり捨てることができることから Disposal Component という名前が適切という話もあります


Blue-Green Deployment
^^^^^^^^^^^^^^^^^^^^^^

[TODO] topicにしたほうがよい？

IIが出てくると「Blue-Green Deployment」という言葉が合わせて出てくることがあります。
「Blue-Green Deployment」を実現するためにIIの考え方を使うと良いというだけなので、密接に関連した概念ではありません。
本番環境に安全にデプロイするための方法です。「継続的デリバリー」にも載っている手法です。
本番環境といえば、ユーザがアクセスするサーバで、デプロイするためには、その本番環境のコードを変更することがあります。
さきほどの「まっしろわーるど」ではありませんが、一つ間違うと障害に直結します。本文から印象的な一文を引用してみましょう。

  万一問題が発生した場合にデプロイメントをロールバックできるようにしておくことが極めて重要だ。障害対応を稼働中の本番環境で進めようとすると、ほぼ間違いなく業務終了後の深夜作業となる。そしてミスを犯して残念な結果を招き、ユーザを怒らせることになるだろう。

本番環境がひとつしかないという場合も多々あると思います。その本番環境にプログラムをデプロイしたその瞬間、「503 Service Temporarily Unavailable」の文字が出現。
目の前がまっしろわーるど [#iimashiro]_ に遭遇した人は私だけではないでしょう [#iitaisho]_ 。
原因は、開発環境と本番環境の違いや、デプロイ職人の人為的ミス、複雑なデプロイの伝達ミス、設定値の変更し忘れ、など多岐にわたります。

.. [#iidep] 現在進行形でそういう運用を行っているところがあると思いますが...
.. [#iimashiro] TVアニメ「未確認で進行形」エンディングテーマ / iTunesでも配信しています
.. [#iitaisho] そう、この記事の読者対象はそういう経験をしたことがあるあなたです

このBlue-Green Deoploymentでは、BlueとGreenと呼ばれる2つの環境を用意します。ユーザからアクセスがある環境をBlue環境とします。
Green環境では、新しいバージョンのソフトウエアがデプロイされており、動作確認が終わったところです。
ユーザからのアクセスをルータによってBlueからGreenに変更することによって、デプロイを完了します。
もし、Green環境で問題が発生した場合、ルータの設定を変更してBlue環境にロールバックします。
こうして被害を最小限にしつつ、残った環境はステージングとしても使用することができます。
ただし、本番の環境が2つ必要になります [#iikanaria]_ 。このサーバを構築するために、IIは必須です。

[TODO]例の画像を突っ込んで解説

.. [#iikanaria] 一部のユーザを新しいバージョンをデプロイしたサーバに振り分けるカナリアリリースという手法も載っています。A/Bテストができたり、徐々に負荷をかけていくテストも行えます


このへんまで書いた
-----------------

このへんまで編集完了



II以前の世界
^^^^^^^^^^^

ここで、インフラあるあるをご覧ください：

* 「あ、明日までに50台構築して。インストール手順は散らばってるからよろしく」「えっ？インストール手順作るの？？」

  * そして、次の日。構築はできたものの、職人の手によるバラツキが・・・

    * セッション数が多いときパフォーマンスがでないぞ？あ、sysctl変更するの忘れてた・・・ポートが枯渇してた・・・

* 別の日、メンテナンスのためサーバを再起動したらアプリケーションが動かなくなりました
* とある日、サーバが物理的に古くなったので新規に構築しようとしたらどこにもドキュメントがなくて、まずは何がインストールされているのか調べる羽目になりました
* ``$ crontab -r`` あ！やっちゃった！！戻さないと。。。バックアップがない！！！
* 「デプロイ職人」という肩書 (察し

繰り返される変更の結果、秘伝のタレが詰まったサーバと化していました [#iinao]_ 。

.. [#iinao] なお、これらはすべてフィクションです。現実に起こった事態とは一切関係はありません



背景
^^^^^

「環境をぶっ壊して、新しく作りなおす」ことが簡単にできる技術が現れたのが、このIIが生まれた背景にあります。

* コードの管理はGit(と、pull request)
* コードのテストにvagrantやdocker、jenkins
* サーバの構築手順はpuppetやchef、ansible
* Amazon Web Service(AWS)といった仮想環境

こういった技術が2014年になってひと通り揃ってきました。


DevOps
^^^^^^^

ここからIIが生まれた理由について、寄り道をします。知ってるよ！ということであれば次の章へ飛んでください。

さて、DevOpsとは、開発（Development）と運用（Operations）のそれぞれの頭文字を取ったものです。悲しいかな、開発と運用は、しばしば対立します。
往々にして、運用は複数のシステムのサーバの面倒をみています。開発者は、問題が見つかったら本番環境でのログが見たいと思います [#iidevlog]_ 。
そのとき運用は、ほかの開発チームからの対応をしており、すぐには対応できないことが多々あります [#iidevops]_ 。
こういったことが積み重なり、開発者はすぐに見たいログが見えない、運用者は複数の開発者からのログ欲しい依頼キューが溜まっていきます [#iidevopsref]_ 。

.. [#iidevlog] 本番のログは秘密がいっぱいで直接見ることができない場合があります
.. [#iidevops] いやーあるんですよねこういう状況。最盛期だと本番へのデプロイを3つ並行しつつ、ログ欲しいよ依頼に対応してたり。え？もちろん聞いた話ですよ？？
.. [#iidevopsref] さらなるDevOpsについては http://www.atmarkit.co.jp/ait/articles/1307/02/news002.html

この状況を打破するために、自動化を図ります。





テスト駆動インフラ
^^^^^^^^^^^^^^^^^

ソフトウエア界では、テスト駆動開発(TDD) [#iitdd]_ という言葉があります。つまり：

* テストを書いて、案の定失敗する
* テストが成功するコードを書く
* リファクタリングをする

というのが基本サイクルです [#iitdd2]_ 。「テスト駆動開発入門」という本がTDDの原典となっています。
まず動くコードを書いて、次にリファクタリングすることで、リファクタされたクリーンでかつ動くコードを作ることができます。テストをパスすることによって進捗が後戻りしないようになります。
コードが大きくなっても、テストにパスしなくてはならないため、バグが少ないコードになることが期待できます。

このテスト駆動開発を、インフラに応用するとどうなるでしょうか。
サーバの状態をチェックするテストを書くことから始まります。例えば、

* apacheがインストールされているかというテストを書きます。実行すると失敗します [#iitkf]_ 
* 何らかの方法でインストールします
* 再度テストを行い、テストが成功することを確認します

そのためにサーバの状態をチェックするためのserverepec [#iiserverspec]_ が登場しました。serverspecについては、後ほどインストールから使い方まで触れます。

.. [#iitdd] test-driven development
.. [#iitdd2] http://ja.wikipedia.org/wiki/インフラ駆動開発
.. [#iitkf] ミニマルでインストールしていたサーバだったとして、ここでは失敗することにしてください
.. [#iiserverspec] http://serverspec.org/


Immutable Infrastructure の利点
-------------------------------

自動化されるとどういうことが起きるかというと、仮想化技術を使って、壊して作りなおすことが簡単になります。自動化により、人の手による設定ミスや漏れがなくなります[特に本番環境に対して有効]。
これは、簡単にサーバを構築できるというインフラの側面だけでなく、ソフトウエアに対しても恩恵があります。
ソフトウエアのテストを行う場合を考えてみます。ひとつのサーバに開発環境が乗っかっている場合、ミドルウエアのバージョンは環境に固定されてしまいます。
このとき、新規にサーバを自動で構築してテストを行うことができるため、ミドルウエアのバージョンは自分で指定することが可能となります。

.. これやるとき、テスト書いてることが前提となっているの

.. herokuの具体例出したほうが早い？かなぁ


IIの三層
--------

「おーけすとれーしょん」「こんふぃぐれーしょん」「ぶーとすとらっぴんぐ」という三層の考え方があります [#iisansou]_ 。
どういう設定をどの層で行うかというのは、議論の余地があり、正確な定義はゆらいでいる状態です。

* Orchestration
  
  * Fabric, Capistrano, MCollective

* Configuration

  * Puppet, Chef, AWS OpsWorks

* Bootstrapping

  * Kickstart, Cobbler, OpenStack, AWS


.. [#iisansou] ひらがなで書いてあるのはなんでかって？その方がかわいいじゃないですか、だそうです(中の人談)[誰]


早速実践しよう
-------------

.. 何を目的としている？

テストを書けよ！
chef辛いという話を聞く。ansibleに鞍替えしてみたい誘惑に駆られる。
が、結局、構築したものがきちんと動いているかどうか確かめる必要がある。だから構築されているサーバに対してserverspecでテストを書くところから始めた。

ここからserverspecの実践を始める。

serverspec
-----------

serverspecとは
^^^^^^^^^^^^^^^

使ってみる
^^^^^^^^^^



docker
--------

dockerとは
^^^^^^^^^^^

使ってみる
^^^^^^^^^

vagrant
--------

vagrantとは
^^^^^^^^^^^

使ってみる
^^^^^^^^^^

ログの管理どうする？
------------------

fluentdを使って収集しましょう。いつでもサーバを壊せる状態にしておきましょう。
Elasticsearch + kibanaでログを可視化できてはっぴー☆

.. fluentdを使う利点とか書く。


DBどうするよ？
-------------

気軽に壊せないので、こわさない。以上！！

サーバの監視どうしよう
--------------------

気軽にこわせて気軽に立ち上がるサーバに名前をつけると大変なことに！！！
サーバに名前を付けることは悪であるという議論。hobbitとかzabbixとかそういうツールだと登録してるホストがなくなるとデータがなくなっちゃうんだよねー過去のトレンドが消えてしまうことが問題
mackerelを取り上げる。



とりまく技術
--------------------

* 概念

  * DevOps
  * 継続的デリバリー

   * 一日に何回デプロイしてますか？
 
  * II
  * blue-green
  * disposable
  * orchestration
  * test
  * 構成管理をcode化するということ

* 技術

  * chef
  * ansible
  * AWS
  * docker
  * vagrant
  * fluentd

    * ログをどうするかの話
    * じゃあDBどうすんのよ。頑張れ！！！

  * Serf
 
    * hostsを書き換える例
 
  * serverspec
  * mackerel.io
  * AMIをコピーするという運用


壮大なメモ
----------

* PhoenixServer : http://martinfowler.com/bliki/PhoenixServer.html

  * フェニックスサーバ。認証監査をしようと思った

    * 今動いている本番環境を再度構築しなおすことになる
    * 定期的にサーバを焼き払ったほうがいい
    * サーバは灰の中から不死鳥のように蘇る。だからフェニックスサーバという
    * 構成のズレ、アドホックな変更でサーバの設定が漂流する。SnowflakeServersにいきつく

      * http://kief.com/configuration-drift.html Configration Drift

    * このような漂流に対向するためにpuppetやchefをつかってサーバを同期し直す。
    * netflixはランダムにサーバを落として大丈夫か試している（ひー

* SnowflakeServer : http://martinfowler.com/bliki/SnowflakeServer.html

  * スノーフレークサーバ。雪のかけらサーバという存在
  * OSやアプリケーションにパッチを当てたりする必要がある
  * 設定を調査すると、サーバによって微妙に違う
  * スキー場にとっては良いが、データセンターではよくない
  * スノーフレークサーバは再現が難しい
  * 本番での障害を開発環境で再現させても調査できない
　
    * 参考文献・目に見えるOpsハンドブック　http://www.amazon.com/gp/product/0975568604
   
  * 芸術家はスノーフレークを好むのだそうだ　http://tatiyants.com/devops-is-ruining-my-craft/
　
    * （サーバ含めそのなかのアプリケーションも工業製品なんだよ！！！わかったか！！！（横暴
    * （昔はひとつのサーバでなんとか出来たけど、今はアクセスも増えてサーバも増えたので芸術品はいらない！！
    * （どーどー落ち着けー、なーー
　
  * スノーフレークのディスクイメージを造ればいいじゃんという議論
  * だがこのディスクイメージはミスや不要な設定も一緒に入っている
  * しかもそれを変更することもある。壊れやすさの真の理由となる（雪だけに
  * 理解や修正がしにくくなる。変更したら影響がどこに及ぶかわからない
  * そんなわけで古代のOSの上に重要なソフトウエアが動作している理由である
  * スノーフレークを避けるためにはpuppetやchefを使って動作の確認のとれたサーバを保持すること
  * レシピを使用すつと、簡単に再構築できる。または、イメージデータを作れる
  * 構成はテキストファイルだから変更はバージョン管理される

  * nologinにしてchefなどからレシピを実行すれば、変更はすべてログに残り監査に対して有効
  * 構成の違いによるバグを減らし、全く同じ環境をつくれる。また、環境の違いに起因するバグを減らせる

    * 継続的デリバリーの本に言及する　あっ

* ConfigurationSynchronization : http://martinfowler.com/bliki/ConfigurationSynchronization.html

  * あんまり重要じゃない

* ImmutableServer : http://martinfowler.com/bliki/ImmutableServer.html

  * やっともどってこれた。この文章からスノーフレークとフェニックスサーバに飛んでいる
  * Netflixが実は実戦でやってたみたい　AMIつくってそれをAWS上に展開している

    * http://techblog.netflix.com/2013/03/ami-creation-with-aminator.html
    * AMIを作るツール　https://github.com/Netflix/aminator#readme


* WEB+DB PRESS 81からメモ

  - IIデメリット　サーバが立ち上がった状態からの変更を禁じているのでちょっとした変更を入れるのにもサーバを作りなおす必要がある
  - サーバの生成廃棄コストが頻繁にあると運用コストが増大する
  - サーバの作成や廃棄が簡単なクラウドを使うのが楽
  - ホストの生成廃棄プロセスをAPIでやれると楽。LBとかもAPIでやれると楽
  - クラスタ監視ツールにmackerel.ioを使おう
  - dokku , flynn, apache mesos, Surf
  - pakker
  - BGDepではLBをAPIで変更できると楽
